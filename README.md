Help Center A/B Test Analysis: Improving Customer Experience
📊 Help Center A/B Test Analysis
This project analyzes an A/B test of two Help Center designs to evaluate their impact on user behavior and overall support experience.

🧪 Goal
Determine which variant (A or B) delivers a better customer experience based on:

Conversion Rate (Did users take action?)

Bounce Rate (Did they leave without interacting?)

Session Duration (How long did they stay?)

🔍 Key Findings
Group B had higher conversions but longer session times.

Group A had a higher bounce rate, indicating users may have left before finding help.

Longer sessions in Group B could suggest either engagement or slower resolution — context matters.

✅ Recommendation
Group B appears more effective in engaging users, but further testing is needed to balance engagement with resolution time.

⚙️ Tools Used
Python, Pandas, Matplotlib

Simulated A/B test dataset

Google Colab
